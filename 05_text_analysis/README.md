# MÃ³dulo de AnÃ¡lisis de Textos Largos con Ollama

Este mÃ³dulo extiende el workshop de extracciÃ³n de informaciÃ³n con capacidades especializadas para analizar textos largos y transcripciones de entrevistas usando modelos LLM locales con Ollama.

## ğŸš€ CaracterÃ­sticas

### Analizador General (`long_text_analyzer.py`)
- **Procesamiento de textos largos**: Maneja documentos extensos dividiendo el contenido en chunks manejables
- **ExtracciÃ³n de palabras clave**: Identifica tÃ©rminos y frases mÃ¡s relevantes
- **Resumen detallado**: Genera sÃ­ntesis comprehensivas manteniendo informaciÃ³n crucial
- **AnÃ¡lisis temÃ¡tico**: Identifica temas principales y estructura del contenido
- **AnÃ¡lisis de sentimientos**: EvalÃºa el tono emocional general
- **DetecciÃ³n de hablantes**: Identifica participantes en transcripciones
- **EstadÃ­sticas**: Conteo de palabras y tiempo de lectura estimado

### Analizador Especializado de Entrevistas (`interview_analyzer.py`)
- **ClasificaciÃ³n de entrevistas**: Identifica el tipo (laboral, periodÃ­stica, acadÃ©mica, etc.)
- **ExtracciÃ³n de insights**: Encuentra revelaciones y puntos clave
- **Citas destacadas**: Identifica declaraciones impactantes y memorables
- **AnÃ¡lisis de preguntas**: Categoriza temas de las preguntas realizadas
- **Estilo de interacciÃ³n**: EvalÃºa dinÃ¡micas comunicacionales
- **EstimaciÃ³n de duraciÃ³n**: Calcula tiempo aproximado de la entrevista

## ğŸ“‹ Requisitos

### Prerequisitos del Sistema
- Python 3.8+
- Ollama instalado y ejecutÃ¡ndose
- Modelo LLM descargado (recomendado: `llama3.1`, `llama3.2`, o `mixtral`)

### Dependencias Python

1. Usando pip:

```bash
# a. con pip
pip install -r requirements.txt
```

2. Usando poetry:

- Se han aplicado cambios en el archivo `pyproject.toml`.
    - aÃ±adido `dependencies = [..., "requests>=2.28.0"]`
- Se puede eliminar el archivo `requirements.txt`.
- Ejecutando el comando de Poetry en tu terminal se instalarÃ¡n las dependencias correctas:

```bash
poetry install
```

## ğŸ”§ InstalaciÃ³n

1. **Instalar Ollama** 

2. **Comprobar que tienes descargado un modelo**

    - para este ejemplo se sugiere `llama3.1`, `llama3.2` o `mixtral`


<!-- REVISAR 

1. **Instalar Ollama** (si no lo tienes):
```bash
# En macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# En Windows, descargar desde https://ollama.ai
```

2. **Descargar un modelo**:

```bash
ollama pull llama3.1
# o
ollama pull llama3.2
# o 
ollama pull mixtral
```

Ya estÃ¡ explicado como descargar gemma3

-->

3. **Clonar los archivos del mÃ³dulo** en tu directorio del workshop:
```
llm-information-extraction-workshop/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ long_text_analyzer.py
â”‚   â”œâ”€â”€ interview_analyzer.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ examples/
    â””â”€â”€ sample_interview.txt
```

4. **Instalar dependencias**:
```bash
cd modules
pip install -r requirements.txt
```

## ğŸ¯ Uso

### AnÃ¡lisis General de Textos Largos

```bash
# AnÃ¡lisis bÃ¡sico
python long_text_analyzer.py mi_documento.txt

# Especificar archivo de salida
python long_text_analyzer.py transcripcion.txt -o reporte_analisis.md

# Usar modelo especÃ­fico
python long_text_analyzer.py documento.txt -m mixtral

# Servidor Ollama personalizado
python long_text_analyzer.py texto.txt --url http://192.168.1.100:11434
```

### AnÃ¡lisis Especializado de Entrevistas

```bash
# AnÃ¡lisis de entrevista
python interview_analyzer.py entrevista.txt

# Con archivo de salida personalizado
python interview_analyzer.py transcripcion_entrevista.txt -o analisis_entrevista.md
```

## ğŸ“„ Formatos de Entrada

### Texto Plano
```
Este es un documento largo que queremos analizar...
El contenido puede incluir mÃºltiples pÃ¡rrafos...
```

### TranscripciÃ³n de Entrevista
```
ENTREVISTADOR: Â¿CuÃ¡l es tu experiencia en el campo?

ENTREVISTADO: Bueno, he trabajado durante mÃ¡s de 10 aÃ±os...

ENTREVISTADOR: Interesante, Â¿podrÃ­as contarme mÃ¡s sobre...
```

## ğŸ“Š Ejemplo de Salida

### Reporte General
```markdown
# REPORTE DE ANÃLISIS DE TEXTO LARGO

## ğŸ“Š ESTADÃSTICAS GENERALES
- **NÃºmero de palabras**: 2,547
- **Tiempo de lectura estimado**: 13 minutos
- **Sentimiento general**: Neutral

## ğŸ¯ PALABRAS CLAVE
inteligencia artificial, machine learning, datos, algoritmos, tecnologÃ­a

## ğŸ·ï¸ TEMAS PRINCIPALES
â€¢ Desarrollo de inteligencia artificial
â€¢ Impacto en la industria tecnolÃ³gica
â€¢ DesafÃ­os Ã©ticos y regulatorios
â€¢ Futuro de la automatizaciÃ³n

## ğŸ“‹ RESUMEN DETALLADO
El documento aborda los avances recientes en inteligencia artificial...
```

### Reporte de Entrevista
```markdown
# REPORTE DE ANÃLISIS DE ENTREVISTA

## ğŸ“Š INFORMACIÃ“N GENERAL
- **Tipo de entrevista**: Entrevista laboral
- **DuraciÃ³n estimada**: 25 minutos
- **Estilo de interacciÃ³n**: Formal colaborativo

## ğŸ’¡ INSIGHTS PRINCIPALES
â€¢ El candidato tiene experiencia sÃ³lida en desarrollo backend
â€¢ Muestra interÃ©s genuino por el aprendizaje continuo
â€¢ Destaca su capacidad de trabajo en equipo

## ğŸ’¬ CITAS DESTACADAS
â€¢ "Mi pasiÃ³n por la tecnologÃ­a me impulsa a estar siempre actualizado"
â€¢ "Creo que la colaboraciÃ³n es clave para el Ã©xito de cualquier proyecto"
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Personalizar ParÃ¡metros del Modelo
Puedes modificar los parÃ¡metros en el cÃ³digo:

```python
analyzer = LongTextAnalyzer(
    model_name="llama3.1",
    ollama_url="http://localhost:11434"
)

# Personalizar parÃ¡metros de la llamada
payload = {
    "model": self.model_name,
    "prompt": prompt,
    "options": {
        "temperature": 0.1,      # Creatividad (0.0-1.0)
        "top_p": 0.9,           # Diversidad de tokens
        "num_ctx": 8192         # Ventana de contexto
    }
}
```

### Ajustar TamaÃ±o de Chunks
```python
# En la clase LongTextAnalyzer
self.max_chunk_size = 4000  # Ajustar segÃºn el modelo
```

## ğŸš¨ SoluciÃ³n de Problemas

### Error de ConexiÃ³n con Ollama
```bash
# Verificar que Ollama estÃ¡ ejecutÃ¡ndose
ollama list

# Iniciar Ollama si estÃ¡ detenido
ollama serve
```

### Modelo No Encontrado
```bash
# Listar modelos disponibles
ollama list

# Descargar modelo necesario
ollama pull llama3.1
```

### Memoria Insuficiente
- Usar modelos mÃ¡s pequeÃ±os: `llama3.2:1b`
- Reducir `max_chunk_size` en el cÃ³digo
- Procesar textos mÃ¡s cortos

### AnÃ¡lisis Lento
- Usar modelos mÃ¡s rÃ¡pidos pero menos precisos
- Reducir el tamaÃ±o de los chunks
- Procesar en paralelo (modificaciÃ³n avanzada)

## ğŸ”„ IntegraciÃ³n con el Workshop

Este mÃ³dulo se integra perfectamente con el workshop existente:

```python
# Ejemplo de uso en un script personalizado
from long_text_analyzer import LongTextAnalyzer
from interview_analyzer import InterviewAnalyzer

# AnÃ¡lisis general
analyzer = LongTextAnalyzer(model_name="llama3.1")
analysis = analyzer.analyze_text(texto_largo)

# AnÃ¡lisis especializado
interview_analyzer = InterviewAnalyzer(model_name="llama3.1")
interview_analysis = interview_analyzer.analyze_interview(transcripcion)
```

## ğŸ¨ Casos de Uso

### InvestigaciÃ³n AcadÃ©mica
- AnÃ¡lisis de transcripciones de focus groups
- Resumen de documentos de investigaciÃ³n largos
- ExtracciÃ³n de temas de entrevistas cualitativas

### Periodismo y Medios
- AnÃ¡lisis de entrevistas periodÃ­sticas
- Resumen de conferencias de prensa
- ExtracciÃ³n de quotes destacados

### Recursos Humanos
- AnÃ¡lisis de entrevistas laborales
- EvaluaciÃ³n de feedback de empleados
- Procesamiento de encuestas abiertas

### ConsultorÃ­a y Negocios
- AnÃ¡lisis de transcripciones de reuniones
- Procesamiento de feedback de clientes
- Resumen de documentos corporativos

## ğŸ› ï¸ Extensibilidad

El mÃ³dulo estÃ¡ diseÃ±ado para ser extensible:

```python
# Crear un analizador personalizado
class CustomAnalyzer(LongTextAnalyzer):
    def custom_analysis(self, text: str):
        # Implementar anÃ¡lisis especÃ­fico
        pass
```

## ğŸ“ˆ Mejoras Futuras

- [ ] Soporte para mÃºltiples idiomas
- [ ] AnÃ¡lisis de emociones mÃ¡s granular
- [ ] DetecciÃ³n automÃ¡tica de temas emergentes
- [ ] IntegraciÃ³n con bases de datos
- [ ] API REST para uso remoto
- [ ] Interfaz web para anÃ¡lisis interactivo

## ğŸ¤ Contribuciones

Este mÃ³dulo es parte del workshop de extracciÃ³n de informaciÃ³n. Para contribuir:

1. Fork del repositorio principal
2. Crear branch para tu feature
3. AÃ±adir tests si es necesario
4. Enviar pull request

## ğŸ“ Licencia

Mantiene la misma licencia que el workshop principal.